---
layout: reveal
title: Deep Attention Recurrent Q-Network, Sorokin, et al.
pres: true
---

Deep Attention Recurrent Q&#8209;Network
=============================================
Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, Anastasiia
Ignateva

Presented by John Stechschulte. Images are taken from the paper, except where
noted.

---

Overview
--------

Updates to deep Atari work (Mnih, et al. 2015):
  * "Attention" network, on top of CNN
  * LSTM recurrent module, in place of fully-connected layers

Better performance on games requiring longer-term strategy (Seaquest) versus
quick responses (Breakout).

--

![DARQN architecture, showing CNN, attention network, and LSTM
module](/res/darqn_arch.png)

---

LSTM
----

Naive idea: recurrent networks have memory . . .
<!-- .element: class="fragment" data-fragment-index="1" -->


. . . but very little.
<!-- .element: class="fragment" data-fragment-index="2" -->


"Long short-term memory" module designed specifically to remember things for
many time steps
<!-- .element: class="fragment" data-fragment-index="3" -->

--

"Long short"???
---------------

RNNs have long term memory stored in the network weights, and short term memory
stored in activations

LSTMs are designed to provide long-lasting activation memory

See Hochreiter, et al. "Long Short-term Memory" 1997.

--

![LSTM module, from colah's blog](/res/LSTM.png)

From [Understanding LSTM
Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by
Christopher Olah

--

![DARQN architecture, showing CNN, attention network, and LSTM
module](/res/darqn_arch.png)

---

Attention network
-----------------

  * Output of CNN is \\(7 \times 7 \times 256\\)
  * Attention network reduces this to one \\(\mathbb{R}^{256}\\) vector
  * Takes input from CNN and LSTM hidden state
  * "Hard" and "soft" versions

--

Soft attention
--------------

--

$$ g(v\_t^i, h\_{t-1}) = \exp(\mathrm{linear}(\tanh(\mathrm{linear}(v\_t^i) + Wh_{t-1}))) / Z $$

  * Two "fully connected" layers, then softmax
  * Network has \\(L = 49\\) outputs
  * \\(h\_{t-1}\\) hidden state from LSTM (at previous timestep)

--

$$ z\_t = \sum\_{i=1}^L g(v\_t^i, h\_{t-1})v\_t^i $$

  * \\(z\_t\\)&mdash;"context vector"
  * \\(v\_t^i\\)&mdash;output vectors of the CNN
  * \\(g(v\_t^i, h\_{t-1})\\)&mdash;weights from attention network


Just a weighted average
<!-- .element: class="fragment" data-fragment-index="1" -->

--

Hard attention
--------------

--

'The "hard" attention mechanism requires sampling only one attention location
from \\(L\\) available at each time step \\(t\\) in accordance with some
stochastic attention policy \\(\pi\_g\\).'

Translation: pick a vector at random, based on some discrete distribution
\\(\pi\_g\\).
<!-- .element: class="fragment" data-fragment-index="1" -->

The discrete distribution is just the output of the same attention network
used in the soft attention system.
<!-- .element: class="fragment" data-fragment-index="2" -->

But backpropagation is more difficult. Requires REINFORCE algorithm to train
attention network.
<!-- .element: class="fragment" data-fragment-index="3" -->

---

Experiments
-----------

--

Four methods compared:
  * DQN: original Atari paper (Mnih, et al.)
  * DRQN: just adding LSTM to DQN (Hausknecht and Stone)
  * DARQN-soft
  * DARQN-hard

--

### Hyperparameters

  * Discount factor, \\(\gamma = 0.99\\)
  * Initial learning rate, 0.01 (soft), 0.001 (hard)
  * Decreases linearly to 0.00025 over 1M steps (both)
  * Exploration, \\(\epsilon\\)-greedy with \\(\epsilon\\) decreasing from 1 to
    0.1 over 1M steps
  * Some others, described in paper

--

### Network sizes&mdash;Seaquest


|            |  DQN | DRQN | hard | soft
|------------|------|------|------|------
| parameters | 1.7M | 1.7M | 845K | 845K



--

|             |   DQN   |  DRQN |  hard |    soft
|-------------|--------:|------:|------:|----------:
| Breakout    | __241__ |    72 |    20 |      11
| Seaquest    | 1,284   | 1,421 | 3,005 | __7,263__
| S. Invaders | __916__ |   571 |   558 |     650
| Tutankham   | __197__ |   181 |   128 |   __197__
| Gopher      | 1,976   | 3,512 | 2,510 | __5,356__

Best average reward per episode of 100 epochs (5M total steps)

--

### Other interesting conclusions

  * Interpretability: can watch attention network output
  * Hard attention "is unable to learn that in order to survive, the submarine
    has to regularly resurface" in Seaquest&mdash;attributed to local minimum
  * LSTM-based methods much worse on Breakout&mdash;possibly due to limited
    roll-out, but longer roll-out didn't make up the difference

---

[Videos](https://www.youtube.com/playlist?list=PLKK-nv55ZMg583wK4Ny5sZu9YoFo27NBi)
